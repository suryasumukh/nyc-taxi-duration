{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('../data')\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, 'train.csv')\n",
    "TEST_FILE = os.path.join(DATA_PATH, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = preprocess(TRAIN_FILE)\n",
    "test_features = preprocess(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_features[[u'vendor_id',\n",
    "       u'passenger_count', u'trip_duration', u'pickup_hr', u'pickup_min', u'pickup_sec',\n",
    "       u'pickup_day', u'pickup_date', u'pickup_mon', u'quarter',\n",
    "       u'weekday', u'holiday', u'h_dist', u's_pickup_latitude',\n",
    "       u's_pickup_longitude', u's_dropoff_latitude', u's_dropoff_longitude',\n",
    "       u'flag', u'month_end', u'month_start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = test_features[[u'vendor_id',\n",
    "       u'passenger_count', u'pickup_hr', u'pickup_min', u'pickup_sec',\n",
    "       u'pickup_day', u'pickup_date', u'pickup_mon', u'quarter',\n",
    "       u'weekday', u'holiday', u'h_dist', u's_pickup_latitude',\n",
    "       u's_pickup_longitude', u's_dropoff_latitude', u's_dropoff_longitude',\n",
    "       u'flag', u'month_end', u'month_start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_features[[u'vendor_id',\n",
    "       u'passenger_count', u'pickup_hr', u'pickup_min', u'pickup_sec',\n",
    "       u'pickup_day', u'pickup_date', u'pickup_mon', u'quarter',\n",
    "       u'weekday', u'holiday', u'h_dist', u's_pickup_latitude',\n",
    "       u's_pickup_longitude', u's_dropoff_latitude', u's_dropoff_longitude',\n",
    "       u'flag', u'month_end', u'month_start']].as_matrix()\n",
    "y_train = train_features['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = test_features.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458644, 19)\n",
      "(1458644,)\n",
      "(625134, 19)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log)))\n",
    "\n",
    "rmsle = root_mean_squared_logarithmic_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_input = Input(shape=(19,))\n",
    "layer1 = Dense(50, activation='relu')(_input)\n",
    "# layer2 = Dense(5, activation='relu')(layer1)\n",
    "_output = Dense(1, activation='linear')(layer1)\n",
    "\n",
    "model = Model(inputs=[_input], outputs=[_output])\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1166915 samples, validate on 291729 samples\n",
      "Epoch 1/300\n",
      "1166915/1166915 [==============================] - 6s - loss: 1.7195 - val_loss: 0.9019\n",
      "Epoch 2/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.8444 - val_loss: 0.8316\n",
      "Epoch 3/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.8268 - val_loss: 0.8227\n",
      "Epoch 4/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.8177 - val_loss: 0.8140\n",
      "Epoch 5/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.8105 - val_loss: 0.8081\n",
      "Epoch 6/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.8055 - val_loss: 0.8037\n",
      "Epoch 7/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.8017 - val_loss: 0.8005\n",
      "Epoch 8/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7989 - val_loss: 0.7980\n",
      "Epoch 9/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7967 - val_loss: 0.7960\n",
      "Epoch 10/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7949 - val_loss: 0.7944\n",
      "Epoch 11/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7935 - val_loss: 0.7932\n",
      "Epoch 12/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7925 - val_loss: 0.7924\n",
      "Epoch 13/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7917 - val_loss: 0.7917\n",
      "Epoch 14/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7912 - val_loss: 0.7911\n",
      "Epoch 15/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7907 - val_loss: 0.7906\n",
      "Epoch 16/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7901 - val_loss: 0.7901\n",
      "Epoch 17/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7897 - val_loss: 0.7896\n",
      "Epoch 18/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7892 - val_loss: 0.7891\n",
      "Epoch 19/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7888 - val_loss: 0.7887\n",
      "Epoch 20/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7884 - val_loss: 0.7883\n",
      "Epoch 21/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7880 - val_loss: 0.7879\n",
      "Epoch 22/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7876 - val_loss: 0.7874\n",
      "Epoch 23/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7872 - val_loss: 0.7870\n",
      "Epoch 24/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7868 - val_loss: 0.7865\n",
      "Epoch 25/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7864 - val_loss: 0.7862\n",
      "Epoch 26/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7859 - val_loss: 0.7857\n",
      "Epoch 27/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7854 - val_loss: 0.7852\n",
      "Epoch 28/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7850 - val_loss: 0.7847\n",
      "Epoch 29/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7845 - val_loss: 0.7842\n",
      "Epoch 30/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7839 - val_loss: 0.7836\n",
      "Epoch 31/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7834 - val_loss: 0.7831\n",
      "Epoch 32/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7828 - val_loss: 0.7825\n",
      "Epoch 33/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7822 - val_loss: 0.7822\n",
      "Epoch 34/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7817 - val_loss: 0.7812\n",
      "Epoch 35/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7810 - val_loss: 0.7806\n",
      "Epoch 36/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7804 - val_loss: 0.7799\n",
      "Epoch 37/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7797 - val_loss: 0.7792\n",
      "Epoch 38/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7789 - val_loss: 0.7785\n",
      "Epoch 39/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7782 - val_loss: 0.7777\n",
      "Epoch 40/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7774 - val_loss: 0.7770\n",
      "Epoch 41/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7766 - val_loss: 0.7761\n",
      "Epoch 42/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7758 - val_loss: 0.7752\n",
      "Epoch 43/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7749 - val_loss: 0.7744\n",
      "Epoch 44/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7741 - val_loss: 0.7734\n",
      "Epoch 45/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7731 - val_loss: 0.7725\n",
      "Epoch 46/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7721 - val_loss: 0.7716\n",
      "Epoch 47/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7711 - val_loss: 0.7704\n",
      "Epoch 48/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7701 - val_loss: 0.7694\n",
      "Epoch 49/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7690 - val_loss: 0.7683\n",
      "Epoch 50/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7679 - val_loss: 0.7671\n",
      "Epoch 51/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7668 - val_loss: 0.7660\n",
      "Epoch 52/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7656 - val_loss: 0.7648\n",
      "Epoch 53/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7643 - val_loss: 0.7635\n",
      "Epoch 54/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7631 - val_loss: 0.7622\n",
      "Epoch 55/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7617 - val_loss: 0.7608\n",
      "Epoch 56/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7604 - val_loss: 0.7595\n",
      "Epoch 57/300\n",
      "1166915/1166915 [==============================] - 4s - loss: 0.7591 - val_loss: 0.7581\n",
      "Epoch 58/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7576 - val_loss: 0.7566\n",
      "Epoch 59/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7562 - val_loss: 0.7552\n",
      "Epoch 60/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7547 - val_loss: 0.7538\n",
      "Epoch 61/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7532 - val_loss: 0.7522\n",
      "Epoch 62/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7517 - val_loss: 0.7506\n",
      "Epoch 63/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7501 - val_loss: 0.7491\n",
      "Epoch 64/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7485 - val_loss: 0.7480\n",
      "Epoch 65/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7470 - val_loss: 0.7460\n",
      "Epoch 66/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7453 - val_loss: 0.7442\n",
      "Epoch 67/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7436 - val_loss: 0.7425\n",
      "Epoch 68/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7419 - val_loss: 0.7412\n",
      "Epoch 69/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7403 - val_loss: 0.7390\n",
      "Epoch 70/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7384 - val_loss: 0.7372\n",
      "Epoch 71/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7367 - val_loss: 0.7354\n",
      "Epoch 72/300\n",
      "1166915/1166915 [==============================] - 2s - loss: 0.7349 - val_loss: 0.7337\n",
      "Epoch 73/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7331 - val_loss: 0.7318\n",
      "Epoch 74/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7312 - val_loss: 0.7298\n",
      "Epoch 75/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7293 - val_loss: 0.7280\n",
      "Epoch 76/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7274 - val_loss: 0.7260\n",
      "Epoch 77/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7255 - val_loss: 0.7241\n",
      "Epoch 78/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7236 - val_loss: 0.7222\n",
      "Epoch 79/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7216 - val_loss: 0.7202\n",
      "Epoch 80/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7196 - val_loss: 0.7182\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166915/1166915 [==============================] - 5s - loss: 0.7176 - val_loss: 0.7162\n",
      "Epoch 82/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7156 - val_loss: 0.7145\n",
      "Epoch 83/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7136 - val_loss: 0.7122\n",
      "Epoch 84/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7115 - val_loss: 0.7101\n",
      "Epoch 85/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7094 - val_loss: 0.7082\n",
      "Epoch 86/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7073 - val_loss: 0.7060\n",
      "Epoch 87/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7052 - val_loss: 0.7039\n",
      "Epoch 88/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7030 - val_loss: 0.7017\n",
      "Epoch 89/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.7008 - val_loss: 0.7000\n",
      "Epoch 90/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6988 - val_loss: 0.6973\n",
      "Epoch 91/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6966 - val_loss: 0.6951\n",
      "Epoch 92/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6944 - val_loss: 0.6930\n",
      "Epoch 93/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6923 - val_loss: 0.6906\n",
      "Epoch 94/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6901 - val_loss: 0.6885\n",
      "Epoch 95/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6880 - val_loss: 0.6863\n",
      "Epoch 96/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6859 - val_loss: 0.6841\n",
      "Epoch 97/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6837 - val_loss: 0.6824\n",
      "Epoch 98/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6815 - val_loss: 0.6809\n",
      "Epoch 99/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6794 - val_loss: 0.6776\n",
      "Epoch 100/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6773 - val_loss: 0.6756\n",
      "Epoch 101/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6752 - val_loss: 0.6737\n",
      "Epoch 102/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6731 - val_loss: 0.6712\n",
      "Epoch 103/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6709 - val_loss: 0.6691\n",
      "Epoch 104/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6688 - val_loss: 0.6670\n",
      "Epoch 105/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6665 - val_loss: 0.6649\n",
      "Epoch 106/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6645 - val_loss: 0.6633\n",
      "Epoch 107/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6627 - val_loss: 0.6607\n",
      "Epoch 108/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6609 - val_loss: 0.6588\n",
      "Epoch 109/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6585 - val_loss: 0.6568\n",
      "Epoch 110/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6566 - val_loss: 0.6547\n",
      "Epoch 111/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6544 - val_loss: 0.6527\n",
      "Epoch 112/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6523 - val_loss: 0.6507\n",
      "Epoch 113/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6502 - val_loss: 0.6484\n",
      "Epoch 114/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6481 - val_loss: 0.6464\n",
      "Epoch 115/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6460 - val_loss: 0.6444\n",
      "Epoch 116/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6440 - val_loss: 0.6420\n",
      "Epoch 117/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6418 - val_loss: 0.6404\n",
      "Epoch 118/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6397 - val_loss: 0.6382\n",
      "Epoch 119/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6377 - val_loss: 0.6359\n",
      "Epoch 120/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6356 - val_loss: 0.6337\n",
      "Epoch 121/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6336 - val_loss: 0.6322\n",
      "Epoch 122/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6315 - val_loss: 0.6296\n",
      "Epoch 123/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6295 - val_loss: 0.6278\n",
      "Epoch 124/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6276 - val_loss: 0.6263\n",
      "Epoch 125/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6256 - val_loss: 0.6236\n",
      "Epoch 126/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6236 - val_loss: 0.6217\n",
      "Epoch 127/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6216 - val_loss: 0.6197\n",
      "Epoch 128/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6196 - val_loss: 0.6185\n",
      "Epoch 129/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6178 - val_loss: 0.6164\n",
      "Epoch 130/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6158 - val_loss: 0.6189\n",
      "Epoch 131/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6139 - val_loss: 0.6121\n",
      "Epoch 132/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6119 - val_loss: 0.6103\n",
      "Epoch 133/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6102 - val_loss: 0.6084\n",
      "Epoch 134/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6082 - val_loss: 0.6080\n",
      "Epoch 135/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6063 - val_loss: 0.6049\n",
      "Epoch 136/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6045 - val_loss: 0.6028\n",
      "Epoch 137/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6027 - val_loss: 0.6011\n",
      "Epoch 138/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.6009 - val_loss: 0.5993\n",
      "Epoch 139/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5991 - val_loss: 0.5973\n",
      "Epoch 140/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5975 - val_loss: 0.5956\n",
      "Epoch 141/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5956 - val_loss: 0.5942\n",
      "Epoch 142/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5940 - val_loss: 0.5922\n",
      "Epoch 143/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5922 - val_loss: 0.5908\n",
      "Epoch 144/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5905 - val_loss: 0.5929\n",
      "Epoch 145/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5889 - val_loss: 0.5873\n",
      "Epoch 146/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5873 - val_loss: 0.5870\n",
      "Epoch 147/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5857 - val_loss: 0.5838\n",
      "Epoch 148/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5841 - val_loss: 0.5844\n",
      "Epoch 149/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5825 - val_loss: 0.5809\n",
      "Epoch 150/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5808 - val_loss: 0.5801\n",
      "Epoch 151/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5792 - val_loss: 0.5776\n",
      "Epoch 152/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5778 - val_loss: 0.5763\n",
      "Epoch 153/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5763 - val_loss: 0.5748\n",
      "Epoch 154/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5749 - val_loss: 0.5730\n",
      "Epoch 155/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5734 - val_loss: 0.5717\n",
      "Epoch 156/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5718 - val_loss: 0.5709\n",
      "Epoch 157/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5706 - val_loss: 0.5690\n",
      "Epoch 158/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5692 - val_loss: 0.5679\n",
      "Epoch 159/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5678 - val_loss: 0.5662\n",
      "Epoch 160/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5665 - val_loss: 0.5647\n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166915/1166915 [==============================] - 5s - loss: 0.5652 - val_loss: 0.5649\n",
      "Epoch 162/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5638 - val_loss: 0.5623\n",
      "Epoch 163/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5626 - val_loss: 0.5609\n",
      "Epoch 164/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5611 - val_loss: 0.5597\n",
      "Epoch 165/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5601 - val_loss: 0.5585\n",
      "Epoch 166/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5588 - val_loss: 0.5570\n",
      "Epoch 167/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5575 - val_loss: 0.5568\n",
      "Epoch 168/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5564 - val_loss: 0.5548\n",
      "Epoch 169/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5555 - val_loss: 0.5558\n",
      "Epoch 170/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5541 - val_loss: 0.5536\n",
      "Epoch 171/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5531 - val_loss: 0.5525\n",
      "Epoch 172/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5519 - val_loss: 0.5503\n",
      "Epoch 173/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5508 - val_loss: 0.5495\n",
      "Epoch 174/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5498 - val_loss: 0.5481\n",
      "Epoch 175/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5487 - val_loss: 0.5473\n",
      "Epoch 176/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5477 - val_loss: 0.5463\n",
      "Epoch 177/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5468 - val_loss: 0.5456\n",
      "Epoch 178/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5459 - val_loss: 0.5443\n",
      "Epoch 179/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5448 - val_loss: 0.5433\n",
      "Epoch 180/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5441 - val_loss: 0.5424\n",
      "Epoch 181/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5432 - val_loss: 0.5414\n",
      "Epoch 182/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5422 - val_loss: 0.5409\n",
      "Epoch 183/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5414 - val_loss: 0.5401\n",
      "Epoch 184/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5405 - val_loss: 0.5389\n",
      "Epoch 185/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5398 - val_loss: 0.5384\n",
      "Epoch 186/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5391 - val_loss: 0.5373\n",
      "Epoch 187/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5382 - val_loss: 0.5365\n",
      "Epoch 188/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5373 - val_loss: 0.5357\n",
      "Epoch 189/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5371 - val_loss: 0.5362\n",
      "Epoch 190/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5360 - val_loss: 0.5345\n",
      "Epoch 191/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5362 - val_loss: 0.5342\n",
      "Epoch 192/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5347 - val_loss: 0.5374\n",
      "Epoch 193/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5341 - val_loss: 0.5335\n",
      "Epoch 194/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5333 - val_loss: 0.5323\n",
      "Epoch 195/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5336 - val_loss: 0.5314\n",
      "Epoch 196/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5321 - val_loss: 0.5307\n",
      "Epoch 197/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5316 - val_loss: 0.5310\n",
      "Epoch 198/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5313 - val_loss: 0.5297\n",
      "Epoch 199/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5305 - val_loss: 0.5316\n",
      "Epoch 200/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5298 - val_loss: 0.5283\n",
      "Epoch 201/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5293 - val_loss: 0.5300\n",
      "Epoch 202/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5286 - val_loss: 0.5274\n",
      "Epoch 203/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5277 - val_loss: 0.5269\n",
      "Epoch 204/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5267 - val_loss: 0.5266\n",
      "Epoch 205/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5265 - val_loss: 0.5244\n",
      "Epoch 206/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5249 - val_loss: 0.5244\n",
      "Epoch 207/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5245 - val_loss: 0.5237\n",
      "Epoch 208/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5247 - val_loss: 0.5241\n",
      "Epoch 209/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5233 - val_loss: 0.5222\n",
      "Epoch 210/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5229 - val_loss: 0.5215\n",
      "Epoch 211/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5224 - val_loss: 0.5208\n",
      "Epoch 212/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5219 - val_loss: 0.5208\n",
      "Epoch 213/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5212 - val_loss: 0.5202\n",
      "Epoch 214/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5209 - val_loss: 0.5196\n",
      "Epoch 215/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5205 - val_loss: 0.5201\n",
      "Epoch 216/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5199 - val_loss: 0.5191\n",
      "Epoch 217/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5196 - val_loss: 0.5186\n",
      "Epoch 218/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5194 - val_loss: 0.5212\n",
      "Epoch 219/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5204 - val_loss: 0.5176\n",
      "Epoch 220/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5244 - val_loss: 0.5178\n",
      "Epoch 221/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5185 - val_loss: 0.5174\n",
      "Epoch 222/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5209 - val_loss: 0.5170\n",
      "Epoch 223/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5175 - val_loss: 0.5166\n",
      "Epoch 224/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5174 - val_loss: 0.5174\n",
      "Epoch 225/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5172 - val_loss: 0.5164\n",
      "Epoch 226/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5168 - val_loss: 0.5182\n",
      "Epoch 227/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5167 - val_loss: 0.5175\n",
      "Epoch 228/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5176 - val_loss: 0.5158\n",
      "Epoch 229/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5162 - val_loss: 0.5153\n",
      "Epoch 230/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5161 - val_loss: 0.5185\n",
      "Epoch 231/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5160 - val_loss: 0.5156\n",
      "Epoch 232/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5158 - val_loss: 0.5153\n",
      "Epoch 233/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5162 - val_loss: 0.5148\n",
      "Epoch 234/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5153 - val_loss: 0.5141\n",
      "Epoch 235/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5154 - val_loss: 0.5165\n",
      "Epoch 236/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5149 - val_loss: 0.5139\n",
      "Epoch 237/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5151 - val_loss: 0.5157\n",
      "Epoch 238/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5150 - val_loss: 0.5147\n",
      "Epoch 239/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5145 - val_loss: 0.5136\n",
      "Epoch 240/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5146 - val_loss: 0.5138\n",
      "Epoch 241/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166915/1166915 [==============================] - 5s - loss: 0.5144 - val_loss: 0.5131\n",
      "Epoch 242/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5146 - val_loss: 0.5133\n",
      "Epoch 243/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5143 - val_loss: 0.5130\n",
      "Epoch 244/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5128\n",
      "Epoch 245/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5147 - val_loss: 0.5157\n",
      "Epoch 246/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5143\n",
      "Epoch 247/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5144 - val_loss: 0.5133\n",
      "Epoch 248/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5138\n",
      "Epoch 249/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5126\n",
      "Epoch 250/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5137 - val_loss: 0.5148\n",
      "Epoch 251/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5144 - val_loss: 0.5131\n",
      "Epoch 252/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5147 - val_loss: 0.5133\n",
      "Epoch 253/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5142 - val_loss: 0.5126\n",
      "Epoch 254/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5135 - val_loss: 0.5123\n",
      "Epoch 255/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5126\n",
      "Epoch 256/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5126\n",
      "Epoch 257/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5133\n",
      "Epoch 258/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5134 - val_loss: 0.5123\n",
      "Epoch 259/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5137 - val_loss: 0.5125\n",
      "Epoch 260/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5141\n",
      "Epoch 261/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5135\n",
      "Epoch 262/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5151\n",
      "Epoch 263/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5142 - val_loss: 0.5121\n",
      "Epoch 264/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5134 - val_loss: 0.5122\n",
      "Epoch 265/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5118\n",
      "Epoch 266/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5121\n",
      "Epoch 267/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5129 - val_loss: 0.5118\n",
      "Epoch 268/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5153\n",
      "Epoch 269/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5134 - val_loss: 0.5144\n",
      "Epoch 270/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5135 - val_loss: 0.5126\n",
      "Epoch 271/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5130 - val_loss: 0.5118\n",
      "Epoch 272/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5132 - val_loss: 0.5123\n",
      "Epoch 273/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5132\n",
      "Epoch 274/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5132 - val_loss: 0.5123\n",
      "Epoch 275/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5129 - val_loss: 0.5226\n",
      "Epoch 276/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5119\n",
      "Epoch 277/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5134 - val_loss: 0.5120\n",
      "Epoch 278/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5117\n",
      "Epoch 279/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5127 - val_loss: 0.5117\n",
      "Epoch 280/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5130 - val_loss: 0.5122\n",
      "Epoch 281/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5131 - val_loss: 0.5125\n",
      "Epoch 282/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5124\n",
      "Epoch 283/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5136 - val_loss: 0.5119\n",
      "Epoch 284/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5140 - val_loss: 0.5115\n",
      "Epoch 285/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5132 - val_loss: 0.5134\n",
      "Epoch 286/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5142 - val_loss: 0.5116\n",
      "Epoch 287/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5127 - val_loss: 0.5114\n",
      "Epoch 288/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5127 - val_loss: 0.5113\n",
      "Epoch 289/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5130 - val_loss: 0.5114\n",
      "Epoch 290/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5134 - val_loss: 0.5120\n",
      "Epoch 291/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5141 - val_loss: 0.5114\n",
      "Epoch 292/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5138 - val_loss: 0.5114\n",
      "Epoch 293/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5124 - val_loss: 0.5114\n",
      "Epoch 294/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5129 - val_loss: 0.5131\n",
      "Epoch 295/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5133 - val_loss: 0.5203\n",
      "Epoch 296/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5130 - val_loss: 0.5124\n",
      "Epoch 297/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5128 - val_loss: 0.5117\n",
      "Epoch 298/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5131 - val_loss: 0.5123\n",
      "Epoch 299/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5131 - val_loss: 0.5116\n",
      "Epoch 300/300\n",
      "1166915/1166915 [==============================] - 5s - loss: 0.5135 - val_loss: 0.5114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nresults = []\\nfor lr, epochs in [(0.0005, 35), (0.0001, 50)]:\\n    optimizer.lr.assign(lr)\\n    result = model.fit(X_train, y_train, \\n                    batch_size=1024,\\n                    epochs=epochs, callbacks=[checkpoint],\\n                    validation_split=0.2)\\n    results.append(result)\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.h5', save_best_only=True,\n",
    "                             monitor='val_loss', verbose=0)\n",
    "results = model.fit(X_train, y_train, \n",
    "                    batch_size=1024,\n",
    "                    epochs=300, callbacks=[checkpoint],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8HPV9//HXZ7Wrwzp8SLJ83zYOtjAG2ZwmHAmHS+Jc\nxCEkAdqUlqTkavmFXA1Nk1/S0KZt2iR+0JQQ+JFgQmhKSsLR4OAcYHzgGywbg7F86b7v3c/vjx0r\nipF2fWi9kvf9fDz02NmZ2Z3PMEZvfec78x1zd0RERABC6S5ARESGD4WCiIj0USiIiEgfhYKIiPRR\nKIiISB+FgoiI9FEoiIhIH4WCiIj0USiIiEifcLoLOFElJSU+Y8aMdJchIjKibNy4sdbdS5OtN+JC\nYcaMGWzYsCHdZYiIjChmtu941tPpIxER6aNQEBGRPgoFERHpM+L6FEQkM/X09FBVVUVnZ2e6SxnW\ncnNzmTJlCpFI5KQ+r1AQkRGhqqqKwsJCZsyYgZmlu5xhyd2pq6ujqqqKmTNnntR36PSRiIwInZ2d\nFBcXKxASMDOKi4tPqTWlUBCREUOBkNyp/jfKmFDYdbiFbz29i9rWrnSXIiIybGVMKOypbuXbz+6h\nvq073aWIyAhVUFCQ7hJSLmNCIRS0qGLu6S1ERGQYy5hQOHqeLRZLcyEiMuK5O3feeScLFy6kvLyc\n1atXA3Do0CEuu+wyzj33XBYuXMhvfvMbotEot9xyS9+6//zP/5zm6hPLmEtS1VIQOXP83c93sPNg\n85B+59mTivjyOxYc17qPPfYYmzdvZsuWLdTW1rJkyRIuu+wyfvSjH3HNNdfwhS98gWg0Snt7O5s3\nb+bAgQNs374dgMbGxiGte6hlTEshFLQUlAkicqp++9vfcuONN5KVlUVZWRlvfetbWb9+PUuWLOEH\nP/gBd999N9u2baOwsJBZs2axd+9e7rjjDp588kmKiorSXX5CGdNSMLUURM4Yx/sX/el22WWXsXbt\nWp544gluueUWPvOZz/CRj3yELVu28NRTT7Fq1SoeeeQR7rvvvnSXOqjMaymkuQ4RGfmWLVvG6tWr\niUaj1NTUsHbtWpYuXcq+ffsoKyvjz//8z/noRz/Kpk2bqK2tJRaL8d73vpevfvWrbNq0Kd3lJ6SW\ngojICXr3u9/N888/z6JFizAzvvnNbzJhwgR++MMfcs899xCJRCgoKOCBBx7gwIED3HrrrcSCq1y+\n/vWvp7n6xDImFP7Qp6BQEJGT09raCsSvZrznnnu45557/mj5zTffzM033/ymzw331kF/GXf6KKZM\nEBEZVMaEQt/pI6WCiMigMi4UFAkiIoPLmFD4w+kjxYKIyGAyLhSUCSIig8ugUIi/qqUgIjK4jAkF\n09VHIiJJpSwUzOw+M6s2s+0J1rnczDab2Q4zey5VtcS3FX/VfQoicjokevbC66+/zsKFC09jNccv\nlS2F+4FrB1toZmOA7wLvdPcFwA0prEV9CiIixyFldzS7+1ozm5FglQ8Cj7n7G8H61amqBdSnIHJG\n+eVdcHjb0H7nhHK47huDLr7rrruYOnUqH//4xwG4++67CYfDrFmzhoaGBnp6evjqV7/KihUrTmiz\nnZ2d3H777WzYsIFwOMy3vvUtrrjiCnbs2MGtt95Kd3c3sViMn/70p0yaNIn3v//9VFVVEY1G+dKX\nvsTKlStPabePlc5hLuYBETP7NVAI/Ku7P5CqjemOZhE5FStXruRTn/pUXyg88sgjPPXUU3ziE5+g\nqKiI2tpaLrzwQt75znf29WEej+985zuYGdu2beOVV17h6quvprKyklWrVvHJT36Sm266ie7ubqLR\nKL/4xS+YNGkSTzzxBABNTU1Dvp/pDIUwcD5wFZAHPG9mL7h75bErmtltwG0A06ZNO6mNaUA8kTNI\ngr/oU2Xx4sVUV1dz8OBBampqGDt2LBMmTODTn/40a9euJRQKceDAAY4cOcKECROO+3t/+9vfcscd\ndwAwf/58pk+fTmVlJRdddBFf+9rXqKqq4j3veQ9z586lvLycv/7rv+azn/0s119/PcuWLRvy/Uzn\n1UdVwFPu3ubutcBaYNFAK7r7ve5e4e4VpaWlJ7UxQwPiicipueGGG3j00UdZvXo1K1eu5KGHHqKm\npoaNGzeyefNmysrK6OzsHJJtffCDH+Txxx8nLy+P5cuX8+yzzzJv3jw2bdpEeXk5X/ziF/nKV74y\nJNvqL52h8N/ApWYWNrNRwAXAy6naWCjYU2WCiJyslStX8vDDD/Poo49yww030NTUxPjx44lEIqxZ\ns4Z9+/ad8HcuW7aMhx56CIDKykreeOMNzjrrLPbu3cusWbP4xCc+wYoVK9i6dSsHDx5k1KhRfOhD\nH+LOO+9MyeirKTt9ZGY/Bi4HSsysCvgyEAFw91Xu/rKZPQlsBWLA99190MtXT5X6FETkVC1YsICW\nlhYmT57MxIkTuemmm3jHO95BeXk5FRUVzJ8//4S/82Mf+xi333475eXlhMNh7r//fnJycnjkkUd4\n8MEHiUQiTJgwgc9//vOsX7+eO++8k1AoRCQS4Xvf+96Q76ONtNMpFRUVvmHDhhP+3J7qFt72rbX8\n242LeceiSSmoTERS6eWXX+Ytb3lLussYEQb6b2VmG929ItlnM/CO5pEVgiIip1MGPnktzYWISMbY\ntm0bH/7wh/9oXk5ODuvWrUtTRcllTCgcvWrY9UQFkRHL3U/oHoB0Ky8vZ/Pmzad1m6faJZAxp4/6\nOppjaS5ERE5Kbm4udXV1uqw8AXenrq6O3Nzck/6OzGkp6OY1kRFtypQpVFVVUVNTk+5ShrXc3Fym\nTJly0p/PmFAIhdSnIDKSRSIRZs6cme4yzngZdPoo/qqWgojI4DImFI4Oc6Gb10REBpcxoXC0paCr\nj0REBpcxoaDHcYqIJJcxoRDS4zhFRJLKoFA4ep+CQkFEZDCZFwrKBBGRQWVMKNDX0SwiIoPJmFBQ\nn4KISHIZFAoaOltEJJkMDIU0FyIiMoxlTChoQDwRkeQyJhT0kB0RkeQyJhRMHc0iIkllTCioT0FE\nJLkMCoX4q/oUREQGlzGhoAHxRESSy5hQgHhrQX0KIiKDS1komNl9ZlZtZtuTrLfEzHrN7H2pqqXf\ntnT1kYhIAqlsKdwPXJtoBTPLAv4BeDqFdfQJmfoUREQSSVkouPtaoD7JancAPwWqU1VHf2amPgUR\nkQTS1qdgZpOBdwPfO13bVJ+CiEhi6exo/hfgs+4eS7aimd1mZhvMbENNTc1JbzBkptNHIiIJhNO4\n7Qrg4eBS0RJguZn1uvvPjl3R3e8F7gWoqKg46d/qIXU0i4gklLZQcPeZR6fN7H7gfwYKhKFk6D4F\nEZFEUhYKZvZj4HKgxMyqgC8DEQB3X5Wq7SauSVcfiYgkkrJQcPcbT2DdW1JVR3+hkKmjWUQkgQy7\no1mXpIqIJJJhoaDTRyIiiWRUKIChSBARGVxGhYJuXhMRSSzDQsGIJb1VTkQkc2VYKKhPQUQkkYwK\nBQ2IJyKSWEaFQigErq5mEZFBZVQoGBr7SEQkkYwKBfUpiIgklmGhoD4FEZFEMioUNCCeiEhiGRUK\nITPUzywiMriMCgW1FEREEsuoUNDjOEVEEsuoUNDNayIiiWVUKGhAPBGRxDIsFNRSEBFJJMNCQS0F\nEZFEMioUUEtBRCShjAoFDXMhIpJYhoWCBsQTEUkkw0JBLQURkUQyKhRMLQURkYRSFgpmdp+ZVZvZ\n9kGW32RmW81sm5n93swWpaqWo9RSEBFJLJUthfuBaxMsfw14q7uXA38P3JvCWgA9ZEdEJJmUhYK7\nrwXqEyz/vbs3BG9fAKakqpajQiG1FEREEkkYCmb2SL/pfzhm2dNDWMefAb9MUMdtZrbBzDbU1NSc\n9EY0IJ6ISGLJWgpz+02//ZhlpUNRgJldQTwUPjvYOu5+r7tXuHtFaenJb1YD4omIJBZOsjzRr9BT\n/vVqZucA3weuc/e6U/2+ZEJ6xo6ISELJQmGUmS0m3qLIC6Yt+Mk7lQ2b2TTgMeDD7l55Kt913NtE\nYx+JiCSSLBQOAd8Kpg/3mz66bFBm9mPgcqDEzKqALwMRAHdfBfwtUAx818wAet294gTrPyHqUxAR\nSSxhKLj7FYMtM7MLknz2xiTLPwp8NGF1Q8zMiMVO5xZFREaWU7kk9SdDVsVpopvXREQSO5VQsCGr\n4jQJ2YgrWUTktDqVUBhxf3Lr5jURkcQS9imY2c8Z+Je/Ee8kHlEM3acgIpJIsquP/vEklw1Lpj4F\nEZGEkl199Fz/92YWARYCB9y9OpWFpYIesiMikliysY9WmdmCYHo0sAV4AHjJzBJecjochUw3r4mI\nJJKso3mZu+8Ipm8FKoOhrs8H/k9KK0uBkMY+EhFJKFkodPebfjvwMwB3P5yyilJJfQoiIgklC4VG\nM7s+GPPoEuBJADMLc4pjH6WD+hRERBJLdvXRXwDfBiYAn+rXQrgKeCKVhaWC7mgWEUks2dVHlQzw\nSE13fwp4KlVFpYoGxBMRSSzZzWvfTrTc3T8xtOWklun0kYhIQslOH/0lsB14BDjICBzvqL/46aN0\nVyEiMnwlC4WJwA3ASqAXWA086u6NqS4sFUz3KYiIJJTw6iN3r3P3VcFzFW4FxgA7zezDp6W6IaY+\nBRGRxJK1FAAws/OAG4nfq/BLYGMqi0oV3bwmIpJYso7mrwB/ArwMPAx8zt17T0dhqaDTRyIiiSVr\nKXwReA1YFPz83+B5yga4u5+T2vKGlm5eExFJLFkozDwtVZwmhm5eExFJJNnNa/sGmm9mIeJ9DAMu\nH65CIfUpiIgkkmzo7CIz+5yZ/buZXW1xdwB7gfefnhKHjh6yIyKSWLLTRw8CDcDzwEeBzxM/C/Mu\nd9+c4tqGnPoUREQSSxYKs4LnJ2Bm3wcOAdPcvTPZF5vZfcD1QLW7LxxguQH/CiwH2oFb3H3TCdZ/\nQkIGPuAjp0VEBJIPnd1zdMLdo0DV8QRC4H4GGEyvn+uAucHPbcD3jvN7T5ruUxARSSxZS2GRmTUH\n0wbkBe+PXpJaNNgH3X2tmc1I8N0rgAc8fuPAC2Y2xswmuvuh4y//xOjqIxGRxJJdfZSVwm1PBvb3\ne18VzEtdKAR9Cu5OcL+FiIj0k+z00bBgZreZ2QYz21BTU3PS3xMKgkCNBRGRgaUzFA4AU/u9nxLM\nexN3v9fdK9y9orS09KQ3GAoaB8oEEZGBpTMUHgc+Etz7cCHQlMr+BIjfvAbqVxARGcxxjZJ6Mszs\nx8DlQImZVQFfBiIA7r4K+AXxy1H3EL8k9dZU1fKHmuKvCgURkYGlLBTc/cYkyx34eKq2PxBDfQoi\nIomMiI7moRJSS0FEJKEMCwW1FEREEsmoUFCfgohIYhkVCkdbChrqQkRkYBkVCkdbCnokp4jIwDIq\nFNRSEBFJLMNCIf6qPgURkYFlVCiYrj4SEUkoo0LhD5ekKhVERAaSYaEQf1WfgojIwDInFF5by5Uv\n3MIE6tSnICIyiMwJhZ4OxtdvpMwaFAoiIoPInFDILwGgxJrU0SwiMogMCoXxAJRYs0JBRGQQGRQK\n8Se2FdOk00ciIoPInFCI5NITLqDUFAoiIoPJnFAAunKKKbEmXZIqIjKIjAqF7txiimnWzWsiIoPI\nrFAIWgqKBBGRgWVWKOSWBKePFAsiIgPJqFCIjSphnLVS3diW7lJERIaljAqFyVOmA7D2pZfTXImI\nyPCUUaEQGTsVgLZdv6KzJ5rmakREhp+MCgXmvI3m0sV8zu/joad+l+5qRESGnZSGgplda2a7zGyP\nmd01wPJpZrbGzF4ys61mtjyV9ZAVpujGH5Cd5ZS/eCcvvlqT0s2JiIw0KQsFM8sCvgNcB5wN3Ghm\nZx+z2heBR9x9MfAB4LupqqfPuJnErv1HloZeoeHBD7N576GUb1JEZKRIZUthKbDH3fe6ezfwMLDi\nmHUcKAqmRwMHU1hPn1FLP0TDJV/iGp4n74dv56eP/j+aO7pPx6ZFRIa1VIbCZGB/v/dVwbz+7gY+\nZGZVwC+AO1JYzx8Z+/a/oeW9P2J8pJP3bv84td9YxNPf/Qzr1j5JU0vr6SpDRGRYCad5+zcC97v7\nP5nZRcCDZrbQ3WP9VzKz24DbAKZNmzZkGy8s/xOYfxVVz90PL/2Iq6v/E579T7p+FabKimmKlNCZ\nW0bXqDI8txjLH0c4fxw5RcXkFpWSP6aEwjFlFBYWEjr6rE8RkRHMUjUOUPBL/m53vyZ4/zkAd/96\nv3V2ANe6+/7g/V7gQnevHux7KyoqfMOGDSmpuavxIPteWkPL3nWEmg+Q23GEwp4aSmJ15FrPoJ/r\n9AjNVkBLaDRtWaPpjIyhJ2cs0dxxkF9MuHA8ucVTKSyZytgJ0xhXpBARkdPLzDa6e0Wy9VLZUlgP\nzDWzmcAB4h3JHzxmnTeAq4D7zewtQC6QtkuCcsZMYt4VN8EVN/3xAnd6utpobaihtbGa9qZauprr\n6G2tpbetHjoayOpsINzVSG5PI2M7X6WwvYkibyVkbw7dei+gLlRMc7iE9tzx9OZPguJZ5JXNYdyU\n+UyaNJmC3Mhp2msRkT9IWSi4e6+Z/RXwFJAF3OfuO8zsK8AGd38c+GvgP8zs08Q7nW/x4TiEqRmR\n3ALGTixg7MSZx/+5WJTO5loaaw7QUvMGHXVV9DYexFoPE2k7zNiuGma0vsbY5gZChx12xD/W7Hns\ntAnU50yhq3A6odK55E8/j0lzz2XSWLUyRCR1Unb6KFVSefooXbyng5bDe6nb/wrth/cQq3uV7OZ9\nFLXvpzR6mDDxu687PUIl06nKm0dnSTmjpp/H1LPOZ+6kYrLDmXUfooicmOM9faRQGO6ivTQd2EXt\nnvV0799Ebs02xrftIt/jg/p1exaVTONA7jy6x5czevZS5pRfxKTioiRfLCKZRKFwJovFiNW/Tt2e\ndTTt3UDW4S2UtLxCobcA0Oq5bA/N50jxEsIzlzH73EuZN3GcTjuJZDCFQqZxp7d+Hwd3/Ja2yucY\nXf0ik7pfB6DNc9hq86kuXkLOnMuYu3gZs8rGYqaQEMkUCgWB1hrqdq6hYeca8g89z8Su1wBo9xw2\nhRZSU7aMMYv+hPMXL6ZIVzuJnNEUCvIm3lZLzfY1NO74X8YeWktpT3xUkb0+kV0FS/HZb2PuBdcy\nZ1KpWhEiZxiFgiTVW72bAxseJ7rrGSY3bSSHbjo8mw1Zi6iecjUl561g6dlzyMvOSnepInKKFApy\nYno6qN/xK2pf+h9Kqv6XcdEaej3Eiyzg9dIrGXv+e7j43AWMztNpJpGRSKEgJ8+d7v0bOPLCT8h7\n9ReUdO0n5sZLPpfKcZczatG7uLiigtLCnHRXKiLHSaEgQ8OdWPUrHF73E0Kv/JwJ7ZUA7IxNZ1vR\nMiIL38UlF11K2ei8NBcqIokoFCQlvP41qtf/lN4dP2di8xZCOK/FythaeBnhc97DBRdfSUlhbrrL\nFJFjKBQk9VqOUL3hMTq2/IzJjesJE2WvT2Tz6LeRc95KLl16IaNHqQ9CZDhQKMhp5e0NHFr3E3pe\nepipzZsI4WyJzWZHyTWMWfJ+li1eSKHuhRBJG4WCpI03VXHodz8itP0nTGivJOrG876QPWXLmXDh\ne3lruS5zFTndFAoyLMSOvMKR3z9IzsuPMa77ID2exTZmU1X6VvIv/UsuXTiTnLACQiTVFAoyvLgT\nfeNFDq3/L3zvr5na/jKtnstzVFA99TpmXbyCi8+aTCRLQ4CLpIJCQYa13v0bqfn1Kopef4r8aBMt\nnsdv7Hxqpy9nzsXv4oK5k8jSqK4iQ0ahICNDtIfuPb+m5oXVjNn3FPmx5nhAhJbQOGM5sy5awZI5\nExUQIqdIoSAjT7SH7t1rOPLCasa+8TQF/QKidtpy5lz8TpbOmUhYp5hETphCQUa2aA+du9dQ88Jq\nxr3xFPmxFpqDgKibvpw5FykgRE6EQkHOHNEeunc/S3XQgjgaEGtDSzgy8z285cLrWDp7vAJCJAGF\ngpyZoj10VT5LzbrVjHvjSUbF2qj3An4fOp+6Gdcz9+J3csHsMvVBiBxDoSBnvp4Ounb+ktqNP2NM\n1bPkx1qo8SI2hcppmHY1My5+HxVzJqkFIYJCQTJNbzddLz9J3YsPU3Dw9xRFG2jxPDbZ2RyYdDUl\nS97HJWfPID8nnO5KRdJCoSCZKxalc/evqX7hYfKr1lLcc5g2z+E5X0x98fmMXXAFlxYeYfTZb4PC\nCemuVuS0GBahYGbXAv8KZAHfd/dvDLDO+4G7AQe2uPsHE32nQkFOiDu9+56n7vcPMOq1pynsqetb\nVJU1hWcXf5ulFUs4q6xQz6WWM1raQ8HMsoBK4O1AFbAeuNHdd/ZbZy7wCHCluzeY2Xh3r070vQoF\nOWnueOM+qjf/kh2H2rmk8hvk0M2e2CSqItNomXwZJZfeQsXsiRpuQ844wyEULgLudvdrgvefA3D3\nr/db55tApbt//3i/V6EgQ6ZxPy2bf0bLjicJN7zK+N5DtHkOv7PFVE+8nNJzrmXJOQsYl5+d7kpF\nTtlwCIX3Ade6+0eD9x8GLnD3v+q3zs+ItyYuIX6K6W53f3KA77oNuA1g2rRp5+/bty8lNUsGc6dz\n9xqqX3iYMW88Q1FvPQCvxKbycv4Semdeyazz3saiibmEc/IhrKCQkeV4QyHdl2KEgbnA5cAUYK2Z\nlbt7Y/+V3P1e4F6ItxROd5GSAczInXcl0+ZdCbEY0cPbOLzpCfJ3P8v1TY8T2fkYHTuywXqpC5ew\nYeGXmVF+CXNnziCkeyLkDJLKUDgATO33fkowr78qYJ279wCvmVkl8ZBYn8K6RBILhciatIjJkxYB\nn4fuNtp2PceRLU+yv7GLc2p/wTWbP07nSxE22Ry6CqZRe/YtzFtYwVlTShUSMqKl8vRRmPipoauI\nh8F64IPuvqPfOtcS73y+2cxKgJeAc929bqDvBPUpyDDQVkfN7nW0bnoUaiuZ2L6LXLpp9jzW2FKa\nSs4nf+4y3rLwfOZPLFJIyLCQ9j6FoIjlwL8Q7y+4z92/ZmZfATa4++MWvwbwn4BrgSjwNXd/ONF3\nKhRk2Gk+RP3ONbRs/yUlh54jP9oEQI2PZpvNo230PNqnX0HZWRdw9rTxjC/MTXPBkomGRSikgkJB\nhjV3qN1Nw661tLy8htzabRR37SeLGF0eZrvPZF94JnmFY6mf9S4mzltM+ZRxlBbmpLtyOcMpFESG\ni44GOvaspWHX78ja/zz5ra+TE20nQi9NPootsdm8GjmLzpKzKZ0wldJ5F7KgLJeS0vHprlzOIAoF\nkeGsrZbOHf9D054XyDq4ibGtu8ki9ker7LTZHCw8h5apV1E4q4JpU6Ywozif7LBurJMTp1AQGUm6\n26FuDx11+6irXMeRlh7GHP4dkzoqyaMLgBov4lWfzJHs6bQXzYbx8yiYvIDJ02Yzo6SAsaMiGqpD\nBqVQEDkTdLfTufs5Gvdtpevwy0TqdzOm/TVGxdr6VmnxPF71idRYCR2542kpnIuXzGVU2WxKJ81g\nWkkhY0ZlU5QbjoeGOyg8Mo5CQeRM5Q6tR4geeYWG/TvoOLCTUP1ustuPUNBVTZ63963a41kcYSwA\nByijNquUK3wdjTlT2DH9JhoL5pJbNpc5E8ZSPKaAceFuwpFciOgKqTONQkEkE7lD4z5itXtpPryH\ntiN76W14g+4o5DftoaCnhldC85jeXUkZ9X/00VbPJZduWiyfg1mTiYXzeH3MxWQXFpM1/izGzL2I\nBZPHkBP0aej+i5FFoSAig4v20nlkN1azk4YDu2lq68BbDtMczaGoZTfh7iYKu2spix3p+0ivh2gn\nl1ZyaWcUe/LKac2dSE/eeLLyCojkjSEnv4jCbAiPm8a4UDu5Ta/yQEM5z1Q28qXrzybmzuVnlZIT\nzkrjzg+xml2w5cdw+eeHbkysIzugeA6Eh+5SZYWCiJwad+hsoru1npbK39C4fydtzfVk9baR01HN\nhNZXKPCWpF9T60W0WCGdsSyaGYWHshkTaqeTHGbHXqfexrAzaz6j88Isan+e3YUXUJM3i65IEZ6V\nw/TWrdQUzqegt4Gu7LE0F87Bwtn05ozFIrmEQiEiIZh6+BksFKZ93AIskoPnjaOgZS8dHuaIjcfD\n2eQUllKQm80o2on0tkFOId2R0YTr95Db04h11NPa4zRMvoqS0Xl09sTojcaY1vkK4ap1tI+eQ8fE\nCxhVUIh3NEN3M6VP/Ck51Vs5uPQL1C/6C3LCISJZIbJCRnc0Rld3lM5ojM6uHkpjtRyxEmaXFRLJ\nCtHdG6OjJ0pHd5TuaCx+ddnORyl44nbaZ11D67vuJ+pGR3eUjp4o4/KzmTg676QOp0JBRFKvuw1a\nq6Gnnd62ejraWmjtdrrr36Czq4v27BLm1T1LlvdQ3dBCgbfR0tpCe6iA/FgzVTlzKYw2MKN9Ozmx\ndtaHzmFRbCcFdPRtosOzybPu07pbnR6hhzAxjBghxlpr37KYGx1kk29dffOqvIQJ1NNOLllEySLW\n9xMyp8lHESXEOGulyUfRwiiiHj8NN9ra6CFMu+eQZTGmWC0HfRyTrJ5mzyObXroJ0+QF7J5xI1f+\n6d+f1D6NlFFSRWQky86HcTOB+C+TwuDnj30IgGnBu3H9lkzrv5o7lxy9KqqnEzoaoKOBvJK5eG0l\n0cIp9HY0E6vZRay3h1h7PbGeTmIxJ9rbRVvZUrqyRxOp3kEsFiXUeoSOvDKywyFG04r1dtLTUkdX\nby9dlkdPOJ+srmayoy1050+mNacMj+RRHKtnVPVGOrt7CZsTwnkjNI6D097JuPa9jKnbgnU20pE3\nnlg4nyghDo5/K+dU/QiLddNLFlE3omQRygqTZZAbayUS62LHqNkUd7xOV1sr5lGyDOpzxhCxKDm9\n7bR2R1lfOJ9Dc1Yy8dAzFNbvwMO5ZNNLbm8TZ581b4gP4JuppSAikgGOt6WgWyNFRKSPQkFERPoo\nFEREpI9CQURE+igURESkj0JBRET6KBRERKSPQkFERPqMuJvXzKwG2HeSHy8BaoewnHTSvgxP2pfh\nSfsC090RfAeEAAAF0ElEQVS9NNlKIy4UToWZbTieO/pGAu3L8KR9GZ60L8dPp49ERKSPQkFERPpk\nWijcm+4ChpD2ZXjSvgxP2pfjlFF9CiIiklimtRRERCSBjAkFM7vWzHaZ2R4zuyvd9ZwoM3vdzLaZ\n2WYz2xDMG2dmz5jZ7uB1bLrrHIiZ3Wdm1Wa2vd+8AWu3uG8Hx2mrmZ2XvsrfbJB9udvMDgTHZrOZ\nLe+37HPBvuwys2vSU/WbmdlUM1tjZjvNbIeZfTKYP+KOS4J9GYnHJdfMXjSzLcG+/F0wf6aZrQtq\nXm1m2cH8nOD9nmD5jFMuwt3P+B8gC3gVmAVkA1uAs9Nd1wnuw+tAyTHzvgncFUzfBfxDuuscpPbL\ngPOA7clqB5YDvwQMuBBYl+76j2Nf7gb+ZoB1zw7+reUAM4N/g1np3oegtonAecF0IVAZ1DvijkuC\nfRmJx8WAgmA6AqwL/ns/AnwgmL8KuD2Y/hiwKpj+ALD6VGvIlJbCUmCPu+91927gYWBFmmsaCiuA\nHwbTPwTelcZaBuXua4H6Y2YPVvsK4AGPewEYY2YTT0+lyQ2yL4NZATzs7l3u/hqwh/i/xbRz90Pu\nvimYbgFeBiYzAo9Lgn0ZzHA+Lu7uRx8IHQl+HLgSeDSYf+xxOXq8HgWuMjv6TNOTkymhMBnY3+99\nFYn/0QxHDjxtZhvN7LZgXpm7HwqmDwNl6SntpAxW+0g9Vn8VnFa5r99pvBGxL8Eph8XE/yod0cfl\nmH2BEXhczCzLzDYD1cAzxFsyje7eG6zSv96+fQmWNwHFp7L9TAmFM8Gl7n4ecB3wcTO7rP9Cj7cf\nR+SlZCO59sD3gNnAucAh4J/SW87xM7MC4KfAp9y9uf+ykXZcBtiXEXlc3D3q7ucCU4i3YOafzu1n\nSigcAKb2ez8lmDdiuPuB4LUa+C/i/1iOHG3CB6/V6avwhA1W+4g7Vu5+JPgfOQb8B384FTGs98XM\nIsR/iT7k7o8Fs0fkcRloX0bqcTnK3RuBNcBFxE/XhYNF/evt25dg+Wig7lS2mymhsB6YG/TgZxPv\nkHk8zTUdNzPLN7PCo9PA1cB24vtwc7DazcB/p6fCkzJY7Y8DHwmudrkQaOp3OmNYOubc+ruJHxuI\n78sHgitEZgJzgRdPd30DCc47/yfwsrt/q9+iEXdcBtuXEXpcSs1sTDCdB7ydeB/JGuB9wWrHHpej\nx+t9wLNBC+/kpbu3/XT9EL96opL4+bkvpLueE6x9FvGrJbYAO47WT/zc4a+A3cD/AuPSXesg9f+Y\nePO9h/j50D8brHbiV198JzhO24CKdNd/HPvyYFDr1uB/0on91v9CsC+7gOvSXX+/ui4lfmpoK7A5\n+Fk+Eo9Lgn0ZicflHOCloObtwN8G82cRD649wE+AnGB+bvB+T7B81qnWoDuaRUSkT6acPhIRkeOg\nUBARkT4KBRER6aNQEBGRPgoFERHpo1AQOYaZRfuNrLnZhnBUXTOb0X+EVZHhJpx8FZGM0+HxYQZE\nMo5aCiLHyeLPtPimxZ9r8aKZzQnmzzCzZ4OB135lZtOC+WVm9l/B2PhbzOzi4KuyzOw/gvHynw7u\nXBUZFhQKIm+Wd8zpo5X9ljW5eznw78C/BPP+Dfihu58DPAR8O5j/beA5d19E/BkMO4L5c4HvuPsC\noBF4b4r3R+S46Y5mkWOYWau7Fwww/3XgSnffGwzAdtjdi82slvgQCj3B/EPuXmJmNcAUd+/q9x0z\ngGfcfW7w/rNAxN2/mvo9E0lOLQWRE+ODTJ+Irn7TUdS3J8OIQkHkxKzs9/p8MP174iPvAtwE/CaY\n/hVwO/Q9OGX06SpS5GTpLxSRN8sLnnx11JPufvSy1LFmtpX4X/s3BvPuAH5gZncCNcCtwfxPAvea\n2Z8RbxHcTnyEVZFhS30KIscp6FOocPfadNcikio6fSQiIn3UUhARkT5qKYiISB+FgoiI9FEoiIhI\nH4WCiIj0USiIiEgfhYKIiPT5/xC1TzDkbehpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125363110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(results.epoch, results.history['loss'], label='loss')\n",
    "plt.plot(results.epoch, results.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = preprocess(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = test_features[[u'id', u'vendor_id',\n",
    "       u'passenger_count', u'pickup_hr', u'pickup_min', u'pickup_sec',\n",
    "       u'pickup_day', u'pickup_date', u'pickup_mon', u'quarter',\n",
    "       u'weekday', u'holiday', u'h_dist', u's_pickup_latitude',\n",
    "       u's_pickup_longitude', u's_dropoff_latitude', u's_dropoff_longitude',\n",
    "       u'flag', u'month_end', u'month_start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.h5')\n",
    "with open('predictions.txt', 'w') as _file:\n",
    "    _file.write('id,trip_duration\\n')\n",
    "    for example in test_features.as_matrix():\n",
    "        tid, pred = example[0], model.predict(np.array([example[1:]]))\n",
    "        pred = pred[0][0]\n",
    "        _file.write('{},{}\\n'.format(tid, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  625135 predictions.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.read_csv('predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>625134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>984.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>815.253960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-151.395401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>577.211197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>738.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1054.804626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>163660.468750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_duration\n",
       "count  625134.000000\n",
       "mean      984.880062\n",
       "std       815.253960\n",
       "min      -151.395401\n",
       "25%       577.211197\n",
       "50%       738.557007\n",
       "75%      1054.804626\n",
       "max    163660.468750"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.458644e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.594923e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.237432e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.970000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.620000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.075000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.526282e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_duration\n",
       "count   1.458644e+06\n",
       "mean    9.594923e+02\n",
       "std     5.237432e+03\n",
       "min     1.000000e+00\n",
       "25%     3.970000e+02\n",
       "50%     6.620000e+02\n",
       "75%     1.075000e+03\n",
       "max     3.526282e+06"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
